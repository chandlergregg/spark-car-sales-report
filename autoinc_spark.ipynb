{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tutorial-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .appName('Car sales report') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fossil-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schema for dataframe\n",
    "schema = StructType([ \\\n",
    "    StructField('incident_id', IntegerType(), False), \\\n",
    "    StructField('incident_type', StringType(), False), \\\n",
    "    StructField('vin', StringType(), False), \\\n",
    "    StructField('make', StringType(), True), \\\n",
    "    StructField('model', StringType(), True), \\\n",
    "    StructField('year', IntegerType(), True), \\\n",
    "    StructField('incident_date', DateType(), False), \\\n",
    "    StructField('description', StringType(), False) \\\n",
    "])\n",
    "\n",
    "# Load dataframe and register as table\n",
    "df = spark.read.format('csv') \\\n",
    "    .schema(schema = schema) \\\n",
    "    .load('data.csv') \\\n",
    "\n",
    "df.registerTempTable(\"auto_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "impossible-stomach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-----------------+--------+------+----+-------------+--------------------+\n",
      "|incident_id|incident_type|              vin|    make| model|year|incident_date|         description|\n",
      "+-----------+-------------+-----------------+--------+------+----+-------------+--------------------+\n",
      "|          1|            I|VXIO456XLBB630221|  Nissan|Altima|2003|   2002-05-08|Initial sales fro...|\n",
      "|          2|            I|INU45KIOOPA343980|Mercedes|  C300|2015|   2014-01-01|Sold from EuroMotors|\n",
      "|          3|            A|VXIO456XLBB630221|    null|  null|null|   2014-07-02|   Head on collision|\n",
      "|          4|            R|VXIO456XLBB630221|    null|  null|null|   2014-08-05| Repair transmission|\n",
      "|          5|            I|VOME254OOXW344325|Mercedes|  E350|2015|   2014-02-01|    Sold from Carmax|\n",
      "|          6|            R|VOME254OOXW344325|    null|  null|null|   2015-02-06|Wheel allignment ...|\n",
      "|          7|            R|VXIO456XLBB630221|    null|  null|null|   2015-01-01|Replace right hea...|\n",
      "|          8|            I|EXOA00341AB123456|Mercedes| SL550|2016|   2015-01-01|   Sold from AceCars|\n",
      "|          9|            A|VOME254OOXW344325|    null|  null|null|   2015-10-01|      Side collision|\n",
      "|         10|            R|VOME254OOXW344325|    null|  null|null|   2015-09-01|       Changed tires|\n",
      "|         11|            R|EXOA00341AB123456|    null|  null|null|   2015-05-01|       Repair engine|\n",
      "|         12|            A|EXOA00341AB123456|    null|  null|null|   2015-05-03|    Vehicle rollover|\n",
      "|         13|            R|VOME254OOXW344325|    null|  null|null|   2015-09-01|Replace passenger...|\n",
      "|         14|            I|UXIA769ABCC447906|  Toyota|Camery|2017|   2016-05-08|Initial sales fro...|\n",
      "|         15|            R|UXIA769ABCC447906|    null|  null|null|   2020-01-02|Initial sales fro...|\n",
      "|         16|            A|INU45KIOOPA343980|    null|  null|null|   2020-05-01|      Side collision|\n",
      "+-----------+-------------+-----------------+--------+------+----+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "killing-representation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-----------+\n",
      "|              vin|make_filled|year_filled|\n",
      "+-----------------+-----------+-----------+\n",
      "|VOME254OOXW344325|   Mercedes|       2015|\n",
      "|UXIA769ABCC447906|     Toyota|       2017|\n",
      "|VXIO456XLBB630221|     Nissan|       2003|\n",
      "|INU45KIOOPA343980|   Mercedes|       2015|\n",
      "|EXOA00341AB123456|   Mercedes|       2016|\n",
      "+-----------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create vin lookup dataframe\n",
    "lookup_df = spark.sql(\"SELECT vin, MAX(make), MAX(year) FROM auto_data GROUP BY vin\") \\\n",
    "    .withColumnRenamed('max(make)', 'make_filled') \\\n",
    "    .withColumnRenamed('max(year)', 'year_filled')\n",
    "\n",
    "lookup_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "certified-session",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----+\n",
      "|incident_type|    make|year|\n",
      "+-------------+--------+----+\n",
      "|            A|Mercedes|2015|\n",
      "|            A|  Nissan|2003|\n",
      "|            A|Mercedes|2015|\n",
      "|            A|Mercedes|2016|\n",
      "+-------------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join lookup table to df to fill in missing data\n",
    "joined_df = df.join(lookup_df, on = 'vin', how = 'inner') \\\n",
    "    .select('incident_type', 'make_filled', 'year_filled') \\\n",
    "    .withColumnRenamed('make_filled', 'make') \\\n",
    "    .withColumnRenamed('year_filled', 'year')\n",
    "\n",
    "# Filter to accident records only, register table\n",
    "accident_df = joined_df.filter(F.col('incident_type') == 'A')\n",
    "accident_df.registerTempTable('accidents')\n",
    "accident_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "administrative-botswana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+\n",
      "|    make|year|accidents|\n",
      "+--------+----+---------+\n",
      "|Mercedes|2015|        2|\n",
      "|Mercedes|2016|        1|\n",
      "|  Nissan|2003|        1|\n",
      "+--------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get count of accidents by make and year\n",
    "result_df = spark.sql(\"SELECT make, year, count(incident_type) as accidents FROM accidents GROUP BY make, year\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pressed-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to output csv file\n",
    "result_df.coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"sep\",\",\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
